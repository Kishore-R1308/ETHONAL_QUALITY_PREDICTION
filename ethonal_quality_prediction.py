# -*- coding: utf-8 -*-
"""Ethonal quality prediction

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1h1NQfOP7CiGnxOUa3BY4dSX_FuYStINO
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score



wine_dataset = pd.read_csv('/content/winequality-red.csv')

wine_dataset.shape

wine_dataset.head()

wine_dataset.isnull().sum()

wine_dataset.describe()

sns.catplot(x='quality', data = wine_dataset, kind = 'count')

plot = plt.figure(figsize=(4,4))
sns.barplot(x='quality', y = 'volatile acidity', data = wine_dataset)

plot = plt.figure(figsize=(5,5))
sns.barplot(x='quality', y = 'citric acid', data = wine_dataset)

correlation = wine_dataset.corr()

plt.figure(figsize=(7,6))
sns.heatmap(correlation, cbar=True, square=True, fmt = '.1f', annot = True, annot_kws={'size':8}, cmap = 'Blues')

X = wine_dataset.drop('quality',axis=1)

print(X)

Y = wine_dataset['quality'].apply(lambda y_value: 1 if y_value>=7 else 0)

print(Y)

"""Train and test"""

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=3)

print(Y.shape, Y_train.shape, Y_test.shape)

"""Model Training:

Random Forest Classifier
"""

model = RandomForestClassifier()

model.fit(X_train, Y_train)

"""Model Evaluation

Accuracy Score:
"""

X_test_prediction = model.predict(X_test)
test_data_accuracy = accuracy_score(X_test_prediction, Y_test)

print('Accuracy : ', test_data_accuracy)

"""Building a Predictive System

"""

input_data = (7.5, 0.5, 0.36, 6.1, 0.071, 17.0, 102.0, 0.9978, 3.35, 0.8, 10.5)

#input_data = (7.3,0.65,0.0,1.2,0.065,15.0,21.0,0.9946,3.39,0.47,10.0)

# changing the input data to a numpy array
input_data_as_numpy_array = np.asarray(input_data)

# reshape the data as we are predicting the label for only one instance
input_data_reshaped = pd.DataFrame([input_data], columns=X.columns)

prediction = model.predict(input_data_reshaped)
print(prediction)

if (prediction[0]==1):
  print('Good Quality Wine')
else:
  print('Bad Quality Wine')

"""--------------------------------------------------------------------------------

2. decision tree model
"""

from sklearn.datasets import make_classification
from sklearn.tree import DecisionTreeClassifier, export_text, plot_tree
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.model_selection import train_test_split
import seaborn as sns
import matplotlib.pyplot as plt

X, y = make_classification(n_samples=1000, n_features=5, n_informative=3,
                           n_redundant=0, n_classes=2, random_state=42)

feature_names = [f"Feature_{i+1}" for i in range(X.shape[1])]

X = wine_dataset.iloc[:, :-1]
y = wine_dataset.iloc[:, -1]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

new_model = DecisionTreeClassifier(criterion='gini', max_depth=4, random_state=42)

new_model.fit(X_train, y_train)

y_pred = new_model.predict(X_test)

cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()



#input_data = (7.5,0.5,0.36,6.1,0.071,17.0,102.0,0.9978,3.35,0.8,10.5)

input_data = (7.3,0.65,0.0,1.2,0.065,15.0,21.0,0.9946,3.39,0.47,10.0)

# changing the input data to a numpy array
input_data_as_numpy_array = np.asarray(input_data)

# reshape the data as we are predicting the label for only one instance
input_data_reshaped = pd.DataFrame([input_data], columns=X.columns)

prediction = new_model.predict(input_data_reshaped)
print(prediction)

if (prediction[0]==1):
  print('Good Quality Wine')
else:
  print('Bad Quality Wine')

# Train Random Forest Model
model_rf = RandomForestClassifier(n_estimators=100, random_state=42)
model_rf.fit(X_train, y_train)

# Train Decision Tree Model
model_dt = DecisionTreeClassifier(random_state=42)
model_dt.fit(X_train, y_train)

import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from sklearn.metrics import classification_report

# Example: Predictions from trained model
y_pred_rf = model_rf.predict(X_test)  # Assuming Random Forest model is trained
y_pred_dt = model_dt.predict(X_test)  # Assuming Decision Tree model is trained

# Generate classification reports
report_rf = classification_report(y_test, y_pred_rf, output_dict=True)
report_dt = classification_report(y_test, y_pred_dt, output_dict=True)

# Extract performance metrics
models = ['Random Forest', 'Decision Tree']
accuracy = [report_rf['accuracy'] * 100, report_dt['accuracy'] * 100]
precision = [report_rf['weighted avg']['precision'] * 100, report_dt['weighted avg']['precision'] * 100]
recall = [report_rf['weighted avg']['recall'] * 100, report_dt['weighted avg']['recall'] * 100]
f1_score = [report_rf['weighted avg']['f1-score'] * 100, report_dt['weighted avg']['f1-score'] * 100]

# Plot the metrics
metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
values = [accuracy, precision, recall, f1_score]
hatch_patterns = ['//', '~', '||', '-']  # Different patterns for each metric

fig, ax = plt.subplots(figsize=(5, 4))

x = np.arange(len(models))  # Labels for models
width = 0.1  # Width of bars

for i in range(len(metrics)):
    bars = ax.bar(x + i * width, values[i], width, label=metrics[i], hatch=hatch_patterns[i], edgecolor='black')

ax.set_xlabel("Models")
ax.set_ylabel("Score (%)")
ax.set_title("Model Performance Metrics")
ax.set_xticks(x + width * 1)
ax.set_xticklabels(models)
ax.legend()

# Save Screenshot
plt.savefig("model_performance_metrics.png", dpi=300, bbox_inches="tight")
plt.show()

import numpy as np
import seaborn as sns

# Get feature importances from Random Forest
feature_importances = model_rf.feature_importances_
feature_names = X.columns

# Create DataFrame for better visualization
feat_importance_df = pd.DataFrame({"Feature": feature_names, "Importance": feature_importances})
feat_importance_df = feat_importance_df.sort_values(by="Importance", ascending=False)

# Plot Feature Importance
plt.figure(figsize=(10, 5))
sns.barplot(x=feat_importance_df["Importance"], y=feat_importance_df["Feature"], palette="viridis")
plt.xlabel("Importance Score")
plt.ylabel("Feature")
plt.title("Feature Importance - Random Forest")
plt.savefig("feature_importance_rf.png", dpi=300)
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Accuracy values
models = ["Random Forest", "Decision Tree"]
accuracy = [93, 82]  # Replace these values with actual accuracy scores

# Define positions for bars to reduce the distance
x_pos = np.arange(len(models))

# Create bar chart with reduced thickness and spacing
plt.figure(figsize=(4, 3))  # Reduce figure width slightly
bars = plt.bar(x_pos, accuracy, color=["blue", "green"], width=0.15)  # Adjust width for reduced spacing

# Display percentage values on top of bars
for bar in bars:
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() - 6,
             f"{bar.get_height()}%", ha="center", fontsize=12, color="white", fontweight="bold")

plt.ylim(0, 100)  # Set y-axis limit from 0 to 100
plt.xticks(x_pos, models)  # Ensure labels align with reduced bar spacing
plt.xlabel("Models")
plt.ylabel("Accuracy (%)")
plt.title("Accuracy Comparison: Random Forest vs Decision Tree")
plt.grid(axis="y", linestyle="--", alpha=0.2)

# Save and show graph
plt.savefig("accuracy_comparison.png", dpi=300)
plt.show()

# Use the binary-labeled target (1 if quality >=7 else 0)
Y = wine_dataset['quality'].apply(lambda y_value: 1 if y_value >= 7 else 0)
X = wine_dataset.drop('quality', axis=1)

# Split data
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)

# Train the Random Forest model with binary labels
model_rf = RandomForestClassifier(n_estimators=100, random_state=42)
model_rf.fit(X_train, Y_train)
from sklearn.metrics import roc_curve, auc

# Get predicted probabilities (class 1: Good Quality)
y_probs_rf = model_rf.predict_proba(X_test)[:, 1]

# Compute ROC curve and AUC
fpr, tpr, thresholds = roc_curve(y_test, y_probs_rf)
roc_auc = auc(fpr, tpr)

# Plot the ROC curve
plt.figure(figsize=(6, 5))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'Random Forest (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - Random Forest (Ethanol/Wine Quality)')
plt.legend(loc='lower right')
plt.grid(alpha=0.3)
plt.savefig("roc_curve_rf.png", dpi=300, bbox_inches="tight")
plt.show()